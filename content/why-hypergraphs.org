[Оригинал (на английском), за авторством Линаса Вепстаса:] (http://blog.opencog.org/2013/03/24/why-hypergraphs/)


* Почему гиперграфы

  OpenCog использует =гиперграфы= для представления информации. Почему?  Я не думаю что кто-то ясно и лаконично объяснил
  это где-то в другом месте, так что я попытаюсь сделать это здесь.

  Это очень важный момент: я не могу передать вам, как много раз я отправлялся на поиски какой-нибудь крутой системы логического программирования или логического вывода или автоматического    доказательства теорем или =движка переписывания графов= или =системы вероятностного программирования= и потом опускал руки и понимал, что
  после множества потерянных часов ничего из этого не делает то, что я хочу. Если вы заинтересованы в AGI, позвольте мне заверить вас: того, что вы хотите, оно тоже не делает.
  Что я, собственно, хочу, чтобы они делали, и почему?

Ладно, начнём с простого -- систем переписывания графов. В наши дни почти все согласны, что перкрасный способ представлять знания -- использовать графы. Структура IsA(Cat, Animal) выглядит как граф с двумя вершинами, Cat и Animal, и подписанным ребром, IsA, между ними. Если помимо этого известно, что IsA(Binky, Cat), то я, в принципе, должен мочь вывести, что IsA(Binky, Animal). Это простое транзитивное отношение, и логическая дедукция в данном примере -- простое правило переписывания графа: если видишь подряд два IsA между первой и последней вершинами нарисуй третье IsA ребро. Просто, да?
И вот вы могли вообразить, что все движки логической индукции и рассуждений в ядрах своих содержат системы переписывания графов, правда? Так вы могли бы подумать. На самом деле, практически нет. А те, что всё же содержат, делают это каким-нибудь внутренним, придуманным на месте, не публичным, недокументированным образом; API нет, наружу оно не выставлено; это не "официальная" часть системы, с которой вы могли бы повозиться.

Ладно, так зачем мне системы переписывания графов? Ну, я работал над парсером естественных языков, так называемым декодером Витерби для Link Grammar. Мой изначальный граф -- цепочка слов, т.е. предложение. Слова -- вершины, а рёбра -- стрелочки, которые называются "следующее слово". Правда просто. Для парсинга предложения я хочу применить определённый набор простых правил переписывания графа: к примеру, если слово X существительное, создать стрелочку "часть речи" (POS) от слова X к особой вершине "существительное". Если слово непосредственно перед X -- прилагательное, (в смысле, у него есть стрелочка типа POS, указывающая на "прилагательное"), создать новую стрелочку, называющуюся "модификатор существительного", указывающую из X на это слово перед ним. Такая разметка графа называется "парсинг зависимостей", и является очень распространённым способом парсинга естественных языков. Так что вы могли бы подумать, что все парсеры зависимостей в ядрах своих содержат систему преписывания графов, да? Едва ли. На самом деле, рпактически ни один. А если и да, то они огорожены, являются сборищами хаков, недокументированы... Ну вы поняли.

Единственный известный мне парсер зависимостей, явным образом работающий на системе переписывания графов, открытой для эспериментов и документированной(!), -- это RelEx. Ух ты! Хотя RelEx изобрёл и использует свою собственную
 кастомную систему переписывания графов, полагаю, что в принципе он мог использовать какую-нибудь другую, уже существующую систему для этих целей ( Ну, вообще не мог, потому что в 2005 никаких устойчивых системы переписывания графов с открытым кодом не было. Ну да ладно.)

Чего ещё я хочу сделать? Ну, я мечтаю об использовании системы машинного обучения для разучивания новых правил! В смысле, цель AGI же в этом, правильно? Иметь машину, могущую изучать новые штуки? Так, чтобы изучать новые правила, посмотрим-ка, нужен простой синтаксис для представления правил. В общем-то, просто графовый язык. И вот вы могли бы подумать, что все системы переписывания графов имеют какой-нибудь простой для понимания и использования графовый язык, да? Ан нет. Подчёркиваю -- чёрта с два. За одним возможным исключением, приходится писать на Джаве или C++ или C#. К сожалению, моя система машинного обучения поа не умеет программировать на этих языках.

Вот прыжок на доверие, на логику, к которому я пришёл: было бы удобно, моги я выразить сами правила переписывания графов как графы. Было бы удобно, если бы я мог выразить логические следствия как графы. Было бы удобно, если бы сам мой графический язык программирования мог быть записан как граф. Вообще, он может. На самом деле, проще всего это сделать, если граф -- гиперграф. В следующем разделе я напишу, почему. Если бы у меня была система переписывания гиперграфов, у меня было бы место, где я мог бы  объединить обработку естественных языков, логические рассуждения и машинное обучение, все в одном. 
И вот вы могли бы подумать, что каждый, старающийся построить систему AGI, писал бы её на основе системы переписывания графов, правильно? Но вы бы ошиблись. Похоже, OpenCog -- единственная система, которая так поступает.
Да, в реализации OpenCog'а куча косяков разработки и архитектурных недостатков. Её трудно понять и трудно использовать. Но теперь, быть может, вы понимаете, почему я присягнул ему на верность вместо того, чтобы убежаь с какой-нибудь ещё системой рассуждений или парсером или байесовой сетью или чем бы то ни было.

Математические основания 

В этом разделе я попытаюсь укрепить все вышеизложенные замечания на прочном математическом основании, воззвав к теории моделей, теории категорий (и даже n-категорий!) и теории типов. Благом от этого будет то, что простейшим способом представить структуры данных так, чтобы алгоритмы машинного обучения могли их выучить и затем применить их к парсингу естественных языков и логическим рассуждениям, является представление структур данных гиперграфами.

В теории моделей и информатике есть идея сигнатуры: множества функций, принимающих некоторое количество аргуметов и возвращающих некоторое значение (совсем как сигнатура в Джаве или C++). Если на минуточку перестать обращать внимание на типы (как это делают lisp и scheme), то, в приницпе, можно подавать любое значение в люой позиции в люой функции, и класть их на стек произвольно, даже рекурсивно. Это называется алгебра термов, или свободная алгебра термов, или "свободная теория". Если функции не имеют имён (являются анонимными), получаем лямбда исчисление.

Один из способ представить эелмент алгебры термов -- в виде графа, а именно направленного дерева. Таким образом, если у нас есть две функции, f(x,y) и g(x,y), и три постоянных a,b,c, то  f(a, g(b,c)) -- двоичное дерево, с f в верхнем узле, g в левом узле и листьями a, b,c. Алгбера терминов тоогда есть просто собрание всех таких деревьев. Ни больше, ни меньше.

Чтобы заниматься полезным программированием, также нужны предикаты или отношения -- штуки со значениями истинности, и термы упорядченности. Так, "больше чем" -- отношение, и "a>b" либо истинно, либо ложно. Отношениями также бывают штуки вроде Является, Имеет, Принадлежит, ЖивётВ, РаботаетВ. Последние два примера должны прояснить, что реляционные алгебры составляют основу баз данных, SQL и noSQL. Отношения совмещаются с логическими операциями (employee X LivesIn city Y AND ReportsTo dept Z  -- классический пример).
   
Вообще, алгебры термов и реляционные алгебры совмещают, так что можно писать что-нибудь вроде 
 3<f(x,y) где f(x,y) терм, < отношение, а 3 постоянная. Добавьте сюда особые операторы, связывающие свободные переменные, Для Всех и СуществуетТакой, и получите логику первого порядка. Так, к примеру, ДляВсех x СуществуетТакой y что  3<f(x,y).

Особым случаем отношение является правило переписывания графа. Это отношение, принимающее терм и замещающее его другим термом. К примеру,  ab->c, что означает ‘встретив строку ab, замени её на c’. BNF нотация в компьютерных языках -- просто набор отношений переписывания термов. Систему переписывания термов используют для парсинга (формального ) языка. Переписывания графов -- просто разновидность этого: встретив граф x, замени его графом y.

So far, I’ve avoided the issue of types.  In programming, types allow type safety.  Types make code more readable: f(string, int) is less mysterious than f(x,y). Types solve certain abstract recursion problems in lambda calculus.  A re-write rule in BNF notation is a typed rewrite rule: a substitution a->bc holds not just for any a, but specifically, only when a is a web page, or an IP address or a URL.  A graph re-write rule that says ‘whenever you see x, replace it with y’ implicitly demands that x be typed: x can’t be just anything, it has to be a specific kind of graph, having a specific shape and linkage.  The rule applies for all graphs that have this shape, that are of this kind or type.  So a re-write rule x->y is really a rule (type x)->(type y). Graphically, its still two points x and y, with a directed edge -> in between them. Oh, wait, x and y aren’t points, x and y are graphs.  What kind of a graph has graphs as points?  What kind of graph has edges between graphs? A hypergraph!

And that is the main Ah-HA! moment.  Once you see that, you start seeing hypergraphs everywhere. Sure, you can visualize Set(a,b,c) as a tree-graph, with Set as the parent node, and three children a,b,c.  Or you can visualize this as a hypergraph: Set as a ‘link’ (a ‘hyper-edge’ with 3 endpoints, not 2), and the points a,b,c as the nodes contained in the link.  In fact, all hypergraphs are dual to these directed trees; if you have one, you can have the other.  Hypergraphs are just a convenient notation.

Lets take a moment to look back on what just happened: a function f(x,y,z) is just a hyperedge f connecting three nodes x,y,z. A boolean expression a AND b AND c can be written as AND(a,b,c), which shows a specific example of a hypergraph equivalance. It can be written as a reduction rule: (a AND b AND c) -> AND(a,b,c) which is itself just a hypergraph of the form x->y with x and y being hypergraphs.  The first-order logic constructs ‘for-all’ and ‘there-exists’ are just special cases of the lambda-calculus binding operation lambda, which binds free variables in an expression. Again, hypergraphs: lambda is just a hyperlink that binds a variable x in an expression y, and y was just a term, ahem, hypergraph!

I mentioned categories and n-categories, and I suppose I should justify this mention. Insofar as category theory is the theory of points and arrows, then a rewrite rule between graphs is a morphism in the category of small diagrams.  A subtle but important point about category theory that is almost never discussed in intro-to-cat-theory texts, is that all objects are implicitly typed. In the the category of Sets, the objects are all of the same kind: they are sets.  Its not mentioned because in a given category, all objects are of the same type; types change only when a functor maps from one to another.   So, to understand the category-theoretic equivalent of types in computer science, we must think of functors.  But, as we just saw, a graph rewriting rule is a morphism between functors.  So you could say that graph re-writing is just the category Cat of small categories.  Or you could slide down this slope in a different direction, and start calling it a 2-category. Whatever.  Perhaps its useful to point out that graph rewriting algorithms are sometimes expressed as being one-pushouts or as being 2-pushouts, with a pushout being a certain category-theoretic concept. Notable, for graph rewriting, is that any category with pushouts and equalizers has all (co-)limits. Except that, as we just saw, we want hyper-graph rewriting systems, not graph rewriting systems. So there.

What else are they good for?

In OpenCog, the Link and Node types inherit from the type Atom. This naming convention is intentionally suggestive: ‘Atom’ is meant to invoke the notion of an ‘atomic formula’ from model theory or first-order logic: that is, a formula that has no variables in it (its fully grounded), and that does have a truth value (its not composed of boolean connectives, and has no quantifiers in it).  This suggestive naming helps establish the intended use of OpenCog hypergraphs with regards to first-order logic.

The truth value is a different matter. The default (simplest) OpenCog truth value is a pair of floating point numbers: a probability and a confidence. These numbers allow several other AI concepts to be mapped into hypegraphs: Bayesian networks, Markov networks, and artificial neural networks. All three of these are graphs: directed graphs, at that. They differ in how they assign and propagate floating-point probabilites, entropies, activations. Ideas such as Markov logic networks, which implement maximum entropy principles (aka Boltzmann parition function) on a network of first-order logic expressions, can be represented with OpenCog hypergraphs.  Oh, and I should mention PLN (Probabilistic Logic Networks), which is what the atomspace was originally designed for. That’s what I like about the OpenCog hypergraph atomspace: it has a tremendously powerful ability to succinctly and easily represent complex modern AI concepts.

The good, the bad and the ugly.

You’ve heard about the good.  Now for the bad and the ugly.  First, the OpenCog atomspace implementation is slow and inefficient, over-complicated, badly architected, weakly-distributed, non-scalable, single-threaded. But lets not go there.  All this might be fixable, after a lot of programming effort (and deep, architectural thinking). Its been hotly debated in the past. Someday, maybe it’ll get fixed.

The bad thing about the OpenCog atomspace is that almost no one understands that, ahem, it is a programming language. Let me be very clear on this: OpenCog implements graph re-writing rules with the ImplicationLink. A sequence of ImplicationLinks can be used to compute things. In that sense, it is somewhat like the language Graph Programs, except that OpenCog allows fractional truth values, and logic programming and other good things.  If we stick to using ImplicationLinks with crisp truth values (T/F), then the resulting system is essentially Prolog. Of course you know that Prolog is popular for AI programming projects, because its fairly easy to write reasoning engines and expert systems and the like in Prolog.  What you may not know is that closely related to Prolog is Answer-Set Programming (ASP) . In fact, ASP uses exactly the same notation as Prolog does. It differs in two important ways: first, when you run a Prolog program, you get one answer. With ASP, you get all of the answers!  Its dramatically more powerful, and the reason for this is that  modern-day ASP solvers are built on top of modern-day Boolean SAT solvers. Which means that they are stunningly efficient and effective.

So what does this have to do with OpenCog? Well, here we have a system that, using ImplicationLinks, is essentially Prolog, more or less, when run in crisp-logic mode. Or, you could say, its like typed Lambda calculus. But do we have a simple, easy-to-use syntax like Prolog for it? No we don’t. That’s bad. Can we take an existing Prolog program, run a tool on it, and convert it to ImplicationLinks? No we don’t.  Would it run fast? No it wouldn’t: it would probably be slower than the slowest Prolog ever: Borland prolog running on a 5MHz IBM PC AT in 1986.  And forget an ASP solver for OpenCog.  For the special case where all OpenCog truth values are crisp T/F values, we do not have a Boolean SAT solver to find solutions for our graphs of ImplicationLinks.  This is bad, Really Bad. But I think that this is because very few people seem to understand that the OpenCog Atomspace really is a petri dish for programming languages.

Heck, we don’t even have anything equivalent to the RelEx Sentence Algorithms for OpenCog, even though RelEx is OpenCog-like. This absence is slowing down my efforts to continue work on the Link-Grammar parser, and to move natural language processing out of its stand-alone arena, into a general, flexible framework.

(And we’ve barely scratched the surface. In order to make implication and pattern mining run quickly in the atomspace, we need to implement something like the concept of ‘memoization‘ from lisp/scheme. But it turns out that memoization is really just a relational algebra: it is a database of short expressions that stand in for long ones. The OpenCog Atomspace is also, among other things, a relational database that can store and query not only flat tables or key-value pairs, but full-blown hypergraphs. And this isn’t a day-dream; its crucial for performance (and its partially implemented)).

Why don’t we have these things? Well, its hard. Its just not easy. We don’t have the infrastructure to make it easy, and we don’t have the users who demand these tools.   I don’t think most users are even aware of what the atomspace could even do.   Almost no one is thinking about ‘how to program in the language of OpenCog’ even though it has the potential of far surpassing any of the existing probabilistic programming languages out there.  Its time to change all this, but it will take someone smart and dedicated to do this. Many someones. This could be you.

